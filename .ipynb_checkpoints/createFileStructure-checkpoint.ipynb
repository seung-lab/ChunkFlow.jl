{
 "metadata": {
  "name": ""
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import matplotlib.pyplot as plt\n",
      "%matplotlib inline\n",
      "import math\n",
      "import numpy"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 1
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "dims = numpy.array([32000 , 32000 , 170])\n",
      "FoV = numpy.array([173 , 173 ,9])\n",
      "overlap = numpy.ceil(FoV/2.0).astype(int)\n",
      "speed=5000\n",
      "\n",
      "\n",
      "divs = numpy.array([2, 2, 1])\n",
      "#We will create cubes of same size L\n",
      "realLongitude =  dims + overlap * 2 * (divs - 1)\n",
      "\n",
      "\n",
      "print realLongitude\n",
      "\n",
      "print \"it will take\" , numpy.prod(realLongitude)/numpy.prod(divs) / (speed*3600) , \" hours \"\n",
      "print \"with efficieny \",  numpy.prod(dims) / float(numpy.prod(realLongitude)) * 100.0 , \"%\""
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "[32174 32174   170]\n",
        "it will take 2444  hours \n",
        "with efficieny  98.9213060492 %\n"
       ]
      }
     ],
     "prompt_number": 22
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import numpy as np\n",
      "import h5py\n",
      "#import tifffile\n",
      "\n",
      "#%% read hdf5 volume\n",
      "def imread( fname ):\n",
      "    if '.hdf5' in fname or '.h5' in fname:\n",
      "        fname = fname.replace(\".hdf5\", \"\")\n",
      "        f = h5py.File( fname )\n",
      "        v = np.asarray( f['/main'] )\n",
      "        f.close()\n",
      "        print 'finished reading image stack :)'\n",
      "        return v\n",
      "    elif '.tif' in fname:\n",
      "#        import skimage.io\n",
      "#        return skimage.io.imread( fname, plugin='tifffile' )  \n",
      "        import tifffile\n",
      "        return tifffile.imread(fname)\n",
      "    else:\n",
      "        print 'file name error, only suport tif and hdf5 now!!!'\n",
      "        \n",
      "\n",
      "def imsave( vol, fname, order='C' ):\n",
      "#    if order=='F':\n",
      "#        vol=vol.transpose((2,1,0))\n",
      "    \n",
      "    if '.hdf5' in fname or '.h5' in fname:\n",
      "        f = h5py.File( fname )\n",
      "        f.create_dataset('/main', data=vol)\n",
      "        f.close()\n",
      "        print 'hdf5 file was written :)'\n",
      "    elif '.tif' in fname:\n",
      "        import skimage.io\n",
      "        skimage.io.imsave(fname, vol, plugin='tifffile')\n",
      "    else:\n",
      "        print 'file name error! only support tif and hdf5 now!!!'\n",
      "        \n",
      "def save_variable( var, vname ):\n",
      "    import pickle\n",
      "    f = open(vname, 'w')\n",
      "    pickle.dump(var, f)\n",
      "    f.close()\n",
      "    \n",
      "def load_variable( vname ):\n",
      "    import pickle\n",
      "    f = open( vname, 'rb' )\n",
      "    var = pickle.load(f)\n",
      "    f.close()\n",
      "    return var\n",
      "\n",
      "def write_for_znn(Dir, vol, cid):\n",
      "    '''transform volume to znn format'''\n",
      "    # make directory\n",
      "    import neupy.os    \n",
      "    neupy.os.mkdir_p(Dir )\n",
      "    neupy.os.mkdir_p(Dir + 'data')\n",
      "    neupy.os.mkdir_p(Dir + 'spec')\n",
      "    vol.tofile(Dir + 'data/' + 'batch'+str(cid)+'.image')\n",
      "    sz = np.asarray(vol.shape)\n",
      "    sz.tofile(Dir + 'data/' + 'batch'+str(cid)+'.size')\n",
      "    \n",
      "    # printf the batch.spec\n",
      "    f = open(Dir + 'spec/' + 'batch'+str(cid)+'.spec', 'w')\n",
      "    f.write('[INPUT1]\\n')\n",
      "    f.write('path=./dataset/piriform/data/batch'+str(cid)+'\\n')\n",
      "    f.write('ext=image\\n')\n",
      "    f.write('size='+str(sz[2])+','+str(sz[1])+','+str(sz[0])+'\\n')\n",
      "    f.write('pptype=standard2D\\n\\n')\n",
      "    "
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#Create an array of location\n",
      "xabs = 0\n",
      "cubes = []\n",
      "\n",
      "xLeft , xRight, yTop, yBottom = 0,0,0,0 \n",
      "\n",
      "for xdiv in range(divs[0]):\n",
      "    \n",
      "    xDivSize = numpy.ceil(dims[0]/divs[0]).astype(int)\n",
      "    \n",
      "    if xdiv == 0: \n",
      "        xLeft = 0\n",
      "    else:\n",
      "        xLeft = xLeft + xDivSize -  overlap[0]\n",
      "    \n",
      "    if xdiv == divs[0]-1:\n",
      "        xRight = dims[0]\n",
      "    else:\n",
      "        xRight = xRight + xDivSize +  overlap[0]\n",
      "\n",
      "    for ydiv in range(divs[1]): \n",
      "        \n",
      "        yDivSize = numpy.ceil(dims[1]/divs[1]).astype(int)\n",
      "        \n",
      "        if ydiv == 0: \n",
      "            yTop = 0\n",
      "        else:\n",
      "            yTop = yTop + yDivSize  -  overlap[1]\n",
      "        \n",
      "        if ydiv == divs[1]-1:\n",
      "            yBottom = dims[1]\n",
      "        else:\n",
      "            yBottom = yBottom + yDivSize +  overlap[1]\n",
      "    \n",
      "        filename = \"x\"+str(xdiv)+\"-y\"+str(ydiv)\n",
      "        cubes.append({'x-min': xLeft, 'x-max':xRight, 'y-min': yTop, 'y-max':yBottom, 'z-min':0 , 'z-max':dims[2],'filename':filename });\n",
      "        "
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 40
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "!rm -rf ./data\n",
      "\n",
      "import os\n",
      "import stat   \n",
      "\n",
      "if not os.path.exists('./data'):\n",
      "     os.makedirs('./data')\n",
      "else:\n",
      "     raise Exception('folder already exists')\n",
      "    \n",
      "    \n",
      "#Create bash file with all the jobs to be run\n",
      "jobs = open('../scheduleJobs.sh','w')\n",
      "\n",
      "memory = 200 #gb\n",
      "   \n",
      "#Divide the dataset\n",
      "for cube in cubes:\n",
      "    print cube\n",
      "    #divide_dataset()\n",
      "    cubePath = './data/'+cube['filename']\n",
      "    os.makedirs(cubePath)\n",
      "    \n",
      "    znn_input = numpy.array([cube['x-max']-cube['x-min'],cube['y-max']-cube['y-min'],cube['z-max']-cube['z-min']])\n",
      "    #Create data specification folder which contains the files for specifiying the dataset\n",
      "    #We have to files, because we run a 2 stage forward pass\n",
      "    os.makedirs(cubePath+'/data_spec')\n",
      "    \n",
      "    #Stage 1\n",
      "    template = \"\"\"[INPUT1]\n",
      "path={path_input}\n",
      "size={size}\n",
      "pptype=standard2D\"\"\"\n",
      "    \n",
      "    context = {\n",
      "     \"path_input\":'./cluster/data/{0}/input'.format(cube['filename']), \n",
      "     \"size\": '{0},{1},{2}'.format(cube['x-max']-cube['x-min'],cube['y-max']-cube['y-min'],cube['z-max']-cube['z-min'])\n",
      "     } \n",
      "    with  open(cubePath+'/data_spec/stage1.1.spec','w') as myfile:\n",
      "        myfile.write(template.format(**context))\n",
      "        \n",
      "    #Stage2\n",
      "    #We don't specify the size for the input 2 because,that's already specify in the output file\n",
      "    template = \"\"\"[INPUT1]\n",
      "path={path_input}\n",
      "size={size}\n",
      "pptype=standard2D\n",
      "\n",
      "[INPUT2]\n",
      "path={path_output}.1\n",
      "offset=54,54,0\n",
      "pptype=transform\n",
      "ppargs=-1,1\"\"\"\n",
      "    \n",
      "    context = {\n",
      "    \"path_input\":'./cluster/data/{0}/input'.format(cube['filename']), \n",
      "    \"size\": '{0},{1},{2}'.format(cube['x-max']-cube['x-min'],cube['y-max']-cube['y-min'],cube['z-max']-cube['z-min']),\n",
      "    \"path_output\":'./cluster/data/{0}/output/stage1'.format(cube['filename']),  #this is the path to outname\n",
      "    } \n",
      "    with  open(cubePath+'/data_spec/stage2.1.spec','w') as myfile:\n",
      "        myfile.write(template.format(**context))\n",
      "        \n",
      "    \n",
      "    \n",
      "    \n",
      "    #Create trainning specification folder which specifies the parameters for the forward pass\n",
      "    #Again we have two files because of the two stage forward pass\n",
      "    os.makedirs(cubePath+'/trainning_spec')\n",
      "    \n",
      "    #Stage 1\n",
      "    fov_stage1 = numpy.array([109,109,1])\n",
      "    outz = (fov_stage1 * (math.pow(memory/43.0 * 55696 / numpy.prod(fov_stage1),1/3.0) - 1)).astype(int)\n",
      "    assert numpy.all(znn_input > outz + fov_stage1 - numpy.array([1,1,1]))\n",
      "\n",
      "    \n",
      "    template = \"\"\"[PATH]\n",
      "config={path_config}\n",
      "load={path_load}\n",
      "data={path_data}\n",
      "save={path_save}\n",
      "\n",
      "[OPTIMIZE]\n",
      "n_threads={threads}\n",
      "force_fft=1\n",
      "optimize_fft=0\n",
      "\n",
      "[TRAIN]\n",
      "test_range=1\n",
      "outsz={output_path_size}\n",
      "softmax=1\n",
      "\n",
      "[MONITOR]\n",
      "check_freq=10\n",
      "test_freq=100\n",
      "\n",
      "[SCAN]\n",
      "outname={outname}\"\"\"\n",
      "    \n",
      "    context = {\n",
      "    \"path_config\":'./cluster/network_spec/VeryDeep2_w109.spec', \n",
      "    \"path_load\":'./cluster/network_instance/VeryDeep2_w109/',\n",
      "    \"path_data\":'./cluster/data/{0}/data_spec/stage1.'.format(cube['filename']),\n",
      "    \"path_save\":'./cluster/data/{0}/output/'.format(cube['filename']),  #this is the path to outname\n",
      "    \"threads\":32,\n",
      "    \"output_path_size\":'{0},{1},{2}'.format(outz[0],outz[1],outz[2]), \n",
      "    \"outname\":'stage1'\n",
      "    } \n",
      "    with  open(cubePath+'/trainning_spec/stage1.spec','w') as myfile:\n",
      "        myfile.write(template.format(**context))\n",
      "    \n",
      "    #Stage2\n",
      "    fov_stage2 = numpy.array([65,65,9])\n",
      "    outz2 = (fov_stage2 * (math.pow(memory/76.0 * 609725 / numpy.prod(fov_stage2),1/3.0) - 1)).astype(int)\n",
      "    \n",
      "    assert numpy.all(znn_input > outz2 + fov_stage2 - numpy.array([1,1,1]))\n",
      "    \n",
      "    template = \"\"\"[PATH]\n",
      "config={path_config}\n",
      "load={path_load}\n",
      "data={path_data}\n",
      "save={path_save}\n",
      "\n",
      "[OPTIMIZE]\n",
      "n_threads={threads}\n",
      "force_fft=1\n",
      "optimize_fft=0\n",
      "\n",
      "[TRAIN]\n",
      "test_range=1\n",
      "outsz={output_path_size}\n",
      "softmax=1\n",
      "\n",
      "[MONITOR]\n",
      "check_freq=10\n",
      "test_freq=100\n",
      "\n",
      "[SCAN]\n",
      "outname={outname}\"\"\"\n",
      "    \n",
      "    context = {\n",
      "    \"path_config\":'./cluster/network_spec/VeryDeep2HR_w65x9.spec', \n",
      "    \"path_load\":'./cluster/network_instance/VeryDeep2HR_w65x9/',\n",
      "    \"path_data\":'./cluster/data/{0}/data_spec/stage2.'.format(cube['filename']),\n",
      "    \"path_save\":'./cluster/data/{0}/output/'.format(cube['filename']),  #this is the path to outname\n",
      "    \"threads\":32,\n",
      "    \"output_path_size\":'{0},{1},{2}'.format(outz2[0],outz2[1],outz2[2]) , \n",
      "    \"outname\":'stage2'\n",
      "    } \n",
      "    with  open(cubePath+'/trainning_spec/stage2.spec','w') as myfile:\n",
      "        myfile.write(template.format(**context))\n",
      "        \n",
      "    #create input and output folders\n",
      "    os.makedirs(cubePath+'/input')\n",
      "    os.makedirs(cubePath+'/output')\n",
      "\n",
      "    #Create a bash files which executes both trainning stages\n",
      "    #This run.sh will be assign to 1 node.\n",
      "    #If we schedule each stage separtely in the cluser, and we have more clusters that cubes\n",
      "    #it would be possible that the stage2 will start executing before the stage 1 finishes.\n",
      "    template = \"\"\"#!/bin/bash\n",
      "    ./znn-release/bin/znn --options={stage1} --test_only=1\n",
      "    ./znn-release/bin/znn --options={stage2} --test_only=1\n",
      "    \"\"\"\n",
      "    \n",
      "    context = {\n",
      "    \"stage1\":'./cluster/data/{0}/trainning_spec/stage2.spec'.format(cube['filename']), \n",
      "    \"stage2\":'./cluster/data/{0}/trainning_spec/stage1.spec'.format(cube['filename'])\n",
      "    } \n",
      "    with  open(cubePath+'/trainning_spec/run.sh','w') as myfile:\n",
      "        myfile.write(template.format(**context))\n",
      "    \n",
      "    #make this file executable\n",
      "    st = os.stat(cubePath+'/trainning_spec/run.sh')\n",
      "    os.chmod(cubePath+'/trainning_spec/run.sh', st.st_mode | 0111 )\n",
      "    \n",
      "    #add \n",
      "    jobs.write('./cluster/data/{0}/trainning_spec/run.sh \\n'.format(cube['filename']))\n",
      "\n",
      "jobs.close()\n",
      "st = os.stat(jobs.name) \n",
      "os.chmod(jobs.name, st.st_mode | 0111 )"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "{'z-min': 0, 'x-min': 0, 'x-max': 16087, 'y-max': 16087, 'filename': 'x0-y0', 'y-min': 0, 'z-max': 170}\n",
        "{'z-min': 0, 'x-min': 0, 'x-max': 16087, 'y-max': 32000, 'filename': 'x0-y1', 'y-min': 15913, 'z-max': 170}\n",
        "{'z-min': 0, 'x-min': 15913, 'x-max': 32000, 'y-max': 48087, 'filename': 'x1-y0', 'y-min': 0, 'z-max': 170}\n",
        "{'z-min': 0, 'x-min': 15913, 'x-max': 32000, 'y-max': 32000, 'filename': 'x1-y1', 'y-min': 15913, 'z-max': 170}\n"
       ]
      }
     ],
     "prompt_number": 137
    }
   ],
   "metadata": {}
  }
 ]
}